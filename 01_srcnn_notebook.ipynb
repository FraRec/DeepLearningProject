{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Imports & Iperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import tf_keras\n",
    "import pathlib\n",
    "import patoolib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import main.models.I_SRCNN.utilities.utils_srcnn as us\n",
    "import main.models.I_SRCNN.utilities.architectures_srcnn as arcs\n",
    "\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from tf_keras.layers import *\n",
    "from tf_keras.optimizers import *\n",
    "from tf_keras.utils import plot_model, img_to_array\n",
    "from tf_keras.preprocessing.image import *\n",
    "from tf_keras.preprocessing import  image_dataset_from_directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Iperparametri** \\\n",
    "Questi sono gli Iperparametri che possiamo decidere noi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SuperRes\n",
    "BATCH_SIZE  = 64\n",
    "SCALE       = 2.0\n",
    "\n",
    "# DataSet\n",
    "INPUT_DIM   = 32\n",
    "LABEL_SIZE  = [20, 64]\n",
    "STRIDE      = 14\n",
    "PAD         = [int((INPUT_DIM - LABEL_SIZE[0]) / 2.0), 0]\n",
    "\n",
    "# Model Training\n",
    "EPOCHS = 200\n",
    "\n",
    "# Random Seed\n",
    "SEED        = 42\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Load Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nell'Articolo Scientifico che descrive la tecnica SRCNN: \"Image Super-Resolution Using Deep\n",
    "Convolutional Networks\", viene esplicitamente detto che per addestrare la rete si è utilizzato un training set relativamente piccolo.\n",
    "\n",
    "> \"we use a relatively small training set,\\\n",
    ">  that consists of 91 images\"\n",
    "\n",
    "In questa implementazione useremo le stesse 91 immagini proposte nell'Articolo.\n",
    "Tali Dati sono contenuti in un archivio che quindi dovrà prima essere estratto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Come prima cosa definiamo il percorso base di questo file\n",
    "current_path = (os.path.dirname(os.path.realpath(os.getcwd())) + \"\\\\\" + os.path.basename(os.getcwd()))\n",
    "current_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estraiamo i Dati dall'archivio nella cartella \".\\data\\SRCNN_Dataset_Train\"\n",
    "data_zip_path = current_path + \"\\images\\SRCNN_Dataset_Train.rar\"\n",
    "data_imgs_path = patoolib.extract_archive(data_zip_path, outdir=(current_path + \"\\\\images\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiamo una lista contenente i percorsi di ogni immagine\n",
    "data_imgs_path = (current_path + \"\\images\\SRCNN_Dataset_Train\")\n",
    "file_train_pattern = (pathlib.Path(data_imgs_path) / \"*.bmp\")\n",
    "file_train_pattern = str(file_train_pattern)\n",
    "dataset_train_paths = [*glob(file_train_pattern)]\n",
    "print(\"number of images:\", len(dataset_train_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plt.imread(np.random.choice(dataset_train_paths))\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "print(img_to_array(img).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Create Augmented Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nell'Articolo viene specificato come le 91 Immagini iniziali vengano successivamente trattate attraverso delle operazioni di \"crop\" e \"downscale\" che servono per creare delle coppie di sotto-immagini della grandezza desiderata le quali andranno a costituire il vero e proprio Training Set con cui verrà addestrata la Rete\n",
    "\n",
    "> \"In the training phase, the ground truth images {Xi}\\\n",
    "> are prepared as sub-images randomly cropped from the training images...\"\\\n",
    "\\\n",
    "> \"...thus the 91-image dataset can be decomposed into 24,800 sub-images,\\\n",
    "> which are extracted from original images\"\n",
    "\n",
    "Nel nostro caso useremo due strade diverse per la creazione di queste coppie di sotto-immagini.\n",
    "Questa differenziazione nasce dalla volontà di creare delle modifiche sulla struttura base dell'architettura SRCNN. Alcune modifiche infatti necessitano degli Iperparametri diversi rispetto ad altre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiamo i percorsi delle Immagini che formeranno il Dataset\n",
    "train_residual_dir_lr = current_path + \"\\\\data\\\\train_residual\\\\lr\"\n",
    "train_residual_dir_hr = current_path + \"\\\\data\\\\train_residual\\\\hr\"\n",
    "train_vanilla_dir_lr = current_path + \"\\\\data\\\\train_vanilla\\\\lr\"\n",
    "train_vanilla_dir_hr = current_path + \"\\\\data\\\\train_vanilla\\\\hr\"\n",
    "train_residual_dir_lr, train_residual_dir_hr, train_vanilla_dir_lr, train_vanilla_dir_hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iperparametri:\n",
    "# - LABEL_SIZE  = 20\n",
    "# - PAD         = int((INPUT_DIM - LABEL_SIZE) / 2)\n",
    "\n",
    "for image_path in tqdm(dataset_train_paths):\n",
    "  filename = pathlib.Path(image_path).stem\n",
    "  image = us.load_img(image_path)\n",
    "  image = us.img_to_array(image)\n",
    "  image = image.astype(np.uint8)\n",
    "  image = us.tight_crop_image(image, SCALE)\n",
    "  scaled = us.downsize_upsize_image(image, SCALE)\n",
    "\n",
    "  height, width = image.shape[:2]\n",
    "  for y in range(0, height - INPUT_DIM + 1, STRIDE):\n",
    "    for x in range(0, width - INPUT_DIM + 1, STRIDE):\n",
    "      input  = us.crop_input(scaled, x, y, INPUT_DIM)\n",
    "      output = us.crop_output(image, x, y, PAD=PAD[0], LABEL_SIZE=LABEL_SIZE[0])\n",
    "\n",
    "      input_path = (f\"\\\\{filename}_{x}_{y}_input.bmp\")\n",
    "      output_path = (f\"\\\\{filename}_{x}_{y}_output.bmp\")\n",
    "\n",
    "      cv2.imwrite(train_vanilla_dir_lr + input_path, cv2.cvtColor(input, cv2.COLOR_BGR2RGB))\n",
    "      cv2.imwrite(train_vanilla_dir_hr + output_path, cv2.cvtColor(output, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iperparametri:\n",
    "# - LABEL_SIZE  = 64\n",
    "# - PAD         = 0\n",
    "\n",
    "for image_path in tqdm(dataset_train_paths):\n",
    "  filename = pathlib.Path(image_path).stem\n",
    "  image = us.load_img(image_path)\n",
    "  image = us.img_to_array(image)\n",
    "  image = image.astype(np.uint8)\n",
    "  image = us.tight_crop_image(image, SCALE)\n",
    "\n",
    "  height, width = image.shape[:2]\n",
    "  for y in range(0, height - INPUT_DIM + 1, STRIDE):\n",
    "    for x in range(0, width - INPUT_DIM + 1, STRIDE):\n",
    "      output = us.crop_output(image, x, y, PAD=PAD[1], LABEL_SIZE=LABEL_SIZE[1])\n",
    "      input  = us.resize_image(output, 1.0/SCALE)\n",
    "      \n",
    "      input_path = (f\"\\\\{filename}_{x}_{y}_input.bmp\")\n",
    "      output_path = (f\"\\\\{filename}_{x}_{y}_output.bmp\")\n",
    "\n",
    "      cv2.imwrite(train_residual_dir_lr + input_path, cv2.cvtColor(input, cv2.COLOR_BGR2RGB))\n",
    "      cv2.imwrite(train_residual_dir_hr + output_path, cv2.cvtColor(output, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Create Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scegli quali coppie di immagini formeranno il Dataset\n",
    "train_dir_lr = [train_vanilla_dir_lr, train_residual_dir_lr]\n",
    "train_dir_hr = [train_vanilla_dir_hr, train_residual_dir_hr]\n",
    "images_to_use = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Set\n",
    "train_data_lr = image_dataset_from_directory(\n",
    "    directory=train_dir_lr[images_to_use],\n",
    "    labels=None,\n",
    "    label_mode=None,\n",
    "    image_size=(INPUT_DIM, INPUT_DIM),\n",
    "    interpolation=\"bilinear\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "train_data_hr = image_dataset_from_directory(\n",
    "    directory=train_dir_hr[images_to_use],\n",
    "    labels=None,\n",
    "    label_mode=None,\n",
    "    image_size=(LABEL_SIZE[0], LABEL_SIZE[0]),\n",
    "    interpolation=\"bilinear\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "ds_train = tf.data.Dataset.zip(train_data_lr, train_data_hr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the iterators\n",
    "i_train = ds_train.as_numpy_iterator();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Train Sample\n",
    "sample_train = i_train.next()\n",
    "us.plot_images([tf.cast(sample_train[1][0], tf.uint8), tf.cast(sample_train[0][0], tf.uint8)]) # HR, LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preprocess Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qui mettiamo tutte le operazioni di perprocessamento dei Dati. Quando vorremo addestrare la rete potremo decidere quale di queste effettuare e quale no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "ds_train = ds_train.map(us.normalizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Create the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scegliamo quale Modello vogliamo Addestrare\n",
    "# (questa scelta dipende anche dal tipo di Dataset che abbiamo creato in precedenza)\n",
    "input_dim = (INPUT_DIM, INPUT_DIM, 3)\n",
    "model = arcs.SRCNN(input_dim=input_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Compile the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scegliamo quale Loss e quale Optimizer assegnare al Modello\n",
    "\n",
    "my_loss = tf_keras.losses.mean_squared_error;\n",
    "my_opt = tf_keras.optimizers.Adam(learning_rate=0.0003);\n",
    "\n",
    "model.compile(\n",
    "  loss=my_loss,\n",
    "  optimizer=my_opt,\n",
    "  metrics=[us.PSNR_metric, us.SSIM_metric]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eseguiamo il Plot del Modello\n",
    "model.summary()\n",
    "plot_model(model, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Fit the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define TensorBoard Callbacks\n",
    "tb_callback = tf_keras.callbacks.TensorBoard(log_dir=(\"logs/\" + str(model._name)), histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addestriamo il Modello sul Dataset\n",
    "history = model.fit(ds_train, epochs=EPOCHS, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **References**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Super-Resolution Convolutional Neural Network for Image Restoration - \"https://medium.datadriveninvestor.com/using-the-super-resolution-convolutional-neural-network-for-image-restoration-ff1e8420d846\"\n",
    "[2] SRCNN-keras - \"https://github.com/MaokeAI/SRCNN-keras/tree/master\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
